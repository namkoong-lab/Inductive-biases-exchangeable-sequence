model_args:
  dim_llm_embedding: 1
  dim_y: 1
  emb_depth: 1
  d_model: 64
  dim_feedforward: 256
  nhead: 4
  dropout: 0.1
  activation: gelu
  num_layers: 4
  bound_std: true
  embed_type: embed_concat
  uncertainty: normal
  loss_type: logprob
  pad_value: 0.0
  gradient_type: full
  model_type: excg

training_args:
  lr: 0.0003
  seed: 3004
  weight_decay: 0.01
  warmup_ratio: 0.03
  min_lr: 0.00003
  total_train_batch_size: 64
  total_test_batch_size: 64
  num_process: 8
  epochs: 400
  eval_steps: 100
  train_horizon: 2000
  test_horizon: 200
  eval_func: "eval_func"
  num_train_samples: 8192
  num_test_samples: 8192
  load_from_checkpoint: false
  checkpoint_path: "/shared/share_mala/Leon/CCB_1/UQ-Horizon-2000_Noise_0.2/model_checkpoint_400.pt"

data_args:
  dataset_name: "gp"
  num_train_workers: 8
  num_test_workers: 8
  noise_scale: 0.1
  dataset_dir: "/user/al4263/rlhf/TPU/data/theta_10_6_4.pt"
  alpha: 0.05

logging_args:
  task: LLM
  wandb_project: GP
  eval_log_step: 10000