{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum user feature ID found: 136\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111002.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111003.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111004.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111005.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111006.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111007.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111008.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111009.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111010.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111011.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111012.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111013.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111014.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111015.gz\n",
      "Processing file: /shared/share_mala/Leon/CB_dataset/dataset/ydata-fp-td-clicks-v2_0.20111016.gz\n",
      "Number of unique articles: 652\n",
      "Original A range: [67124, 615546]\n",
      "Indexed A range: [0, 651]\n",
      "\n",
      "Final dataset statistics:\n",
      "X_train shape: (22222156, 137)\n",
      "X_test shape: (5555539, 137)\n",
      "Y_train shape: (22222156,)\n",
      "Y_test shape: (5555539,)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def parse_yahoo_r6_line(line):\n",
    "    parts = line.strip().split('|')\n",
    "    header_part = parts[0].strip().split()\n",
    "    timestamp = int(header_part[0])\n",
    "    displayed_article = int(header_part[1].replace('id-', ''))\n",
    "    click = int(header_part[2])\n",
    "\n",
    "    user_part = parts[1].strip().split()\n",
    "    # user_part[0] = 'user', the rest are user features (categorical IDs)\n",
    "    user_features = list(map(int, user_part[1:]))\n",
    "\n",
    "    pool_article_ids = []\n",
    "    for article_section in parts[2:]:\n",
    "        article_id_str = article_section.strip()\n",
    "        article_id = int(article_id_str.replace('id-', ''))\n",
    "        pool_article_ids.append(article_id)\n",
    "\n",
    "    return timestamp, displayed_article, click, user_features, pool_article_ids\n",
    "\n",
    "# Path to your dataset directory\n",
    "data_path = \"/shared/share_mala/Leon/CB_dataset/dataset/\"\n",
    "gz_files = sorted(glob.glob(os.path.join(data_path, \"ydata-fp-td-clicks-v2_0.*.gz\")))\n",
    "\n",
    "all_contexts = []\n",
    "all_actions = []\n",
    "all_rewards = []\n",
    "all_pool_article_ids = []\n",
    "\n",
    "# First pass: Collect all user features to determine max ID\n",
    "max_user_feature_id = 0\n",
    "for gz_file in gz_files:\n",
    "    with gzip.open(gz_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            if not line.strip() or not line[0].isdigit():\n",
    "                continue\n",
    "            if '|user' not in line:\n",
    "                continue\n",
    "            _, _, _, user_features, _ = parse_yahoo_r6_line(line)\n",
    "            if user_features:\n",
    "                max_id = max(user_features)\n",
    "                if max_id > max_user_feature_id:\n",
    "                    max_user_feature_id = max_id\n",
    "\n",
    "print(\"Maximum user feature ID found:\", max_user_feature_id)\n",
    "\n",
    "# Second pass: One-hot encode\n",
    "for gz_file in gz_files:\n",
    "    print(\"Processing file:\", gz_file)\n",
    "    with gzip.open(gz_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            if not line.strip() or not line[0].isdigit():\n",
    "                continue\n",
    "            if '|user' not in line:\n",
    "                continue\n",
    "\n",
    "            timestamp, displayed_article, click, user_features, pool_article_ids = parse_yahoo_r6_line(line)\n",
    "\n",
    "            # Create a one-hot vector for user features\n",
    "            one_hot_user = np.zeros(max_user_feature_id, dtype=np.float32)\n",
    "            for uf in user_features:\n",
    "                one_hot_user[uf - 1] = 1.0\n",
    "\n",
    "            all_contexts.append(one_hot_user)\n",
    "            all_actions.append(displayed_article)\n",
    "            all_rewards.append(click)\n",
    "            all_pool_article_ids.append(pool_article_ids)\n",
    "\n",
    "X = np.array(all_contexts, dtype=np.float32)\n",
    "A = np.array(all_actions, dtype=np.int32)\n",
    "Y = np.array(all_rewards, dtype=np.int32)\n",
    "\n",
    "# Pad the candidate articles\n",
    "if all_pool_article_ids:\n",
    "    max_articles = max(len(p) for p in all_pool_article_ids)\n",
    "    padded_article_ids = [p + [-1]*(max_articles - len(p)) for p in all_pool_article_ids]\n",
    "    P = np.array(padded_article_ids, dtype=np.int32)\n",
    "else:\n",
    "    P = np.empty((0,0), dtype=np.int32)\n",
    "\n",
    "# Create output directory\n",
    "out_dir = \"/shared/share_mala/Leon/yahoo_cb\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Create article ID to index mapping\n",
    "unique_articles = sorted(np.unique(A))\n",
    "article_to_idx = {int(article_id): idx for idx, article_id in enumerate(unique_articles)}\n",
    "\n",
    "# Save the mapping\n",
    "with open(os.path.join(out_dir, \"article_to_idx.json\"), 'w') as f:\n",
    "    json.dump(article_to_idx, f)\n",
    "\n",
    "# Convert A to use indices\n",
    "A_indexed = np.array([article_to_idx[a] for a in A], dtype=np.int32)\n",
    "\n",
    "# Save the indexed version\n",
    "np.save(os.path.join(out_dir, \"A_indexed.npy\"), A_indexed)\n",
    "\n",
    "print(f\"Number of unique articles: {len(unique_articles)}\")\n",
    "print(f\"Original A range: [{A.min()}, {A.max()}]\")\n",
    "print(f\"Indexed A range: [0, {len(unique_articles)-1}]\")\n",
    "\n",
    "# Create final dataset by concatenating X and A_indexed\n",
    "X_final = np.column_stack((X, A_indexed))\n",
    "\n",
    "# Create train-test split (80-20 split)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_final, Y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Save the train-test splits\n",
    "split_dir = os.path.join(out_dir, \"splits\")\n",
    "os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(split_dir, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(split_dir, \"X_test.npy\"), X_test)\n",
    "np.save(os.path.join(split_dir, \"Y_train.npy\"), Y_train)\n",
    "np.save(os.path.join(split_dir, \"Y_test.npy\"), Y_test)\n",
    "\n",
    "print(\"\\nFinal dataset statistics:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
