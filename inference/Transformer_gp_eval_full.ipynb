{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rlLel-ZKX7kq"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from torch.distributions import Normal\n",
        "from utils.load_model import load_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2aMVGWX-YHMy"
      },
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "\n",
        "\n",
        "seed = 42\n",
        "\n",
        "# Set the seed for PyTorch\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Set the seed for NumPy\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set the seed for Python's built-in random module\n",
        "random.seed(seed)\n",
        "\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZXIkm1t9YVmR"
      },
      "outputs": [],
      "source": [
        "# Load trained moddels\n",
        "\n",
        "Checkpoint_iter = 100\n",
        "dim_llm_embedding = 4\n",
        "\n",
        "AUTO_REG_CKPT_PATH = f\"/shared/share_mala/Leon/GP_{dim_llm_embedding}/autoreg-UQ-normal-Gradient-full-Loss-logprob-Horizon-2000_Noise_0.1/model_checkpoint_{Checkpoint_iter}.pt\"\n",
        "EXCG_CKPT_PATH = f\"/shared/share_mala/Leon/GP_{dim_llm_embedding}/excg-UQ-normal-Gradient-full-Loss-logprob-Horizon-2000_Noise_0.1/model_checkpoint_{Checkpoint_iter}.pt\"\n",
        "Auto_reg_config_path = f\"../scripts/gp_uq_normal_autoreg.yaml\"\n",
        "Excg_config_path = f\"../scripts/gp_uq_normal_excg.yaml\"\n",
        "\n",
        "auto_reg_model = load_model(Auto_reg_config_path, AUTO_REG_CKPT_PATH, \"autoreg\", device)\n",
        "excg_model = load_model(Excg_config_path, EXCG_CKPT_PATH, \"excg\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDGnhpJwYeT0"
      },
      "outputs": [],
      "source": [
        "# Generate new data on which we want to perform evaluation\n",
        "\n",
        "# We should be careful of seeds here, we should not choose the seeds on which model was trained\n",
        "\n",
        "# Set the params same as we trained on\n",
        "\n",
        "\n",
        "from data.load_data import GPSamplerConstantDataset, scalar_collate_fn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "B = 10   #number of sequences on which we want to evaluate\n",
        "noise = 0.1\n",
        "test_horizon = 100\n",
        "num_test_samples = 8192\n",
        "mean_constant = 0.0\n",
        "lengthscale = 1.0\n",
        "outputscale = 1.0\n",
        "x_range = (-2,2)\n",
        "\n",
        "eval_dataset = GPSamplerConstantDataset(\n",
        "        num_samples=num_test_samples, \n",
        "        noise = noise,\n",
        "        dimension=dim_llm_embedding, \n",
        "        horizon=test_horizon)\n",
        "\n",
        "eval_data_loader = DataLoader(eval_dataset, batch_size=B, shuffle=True, collate_fn=scalar_collate_fn)\n",
        "batch = next(iter(eval_data_loader))\n",
        "print(batch.x.shape, batch.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O61POxaYa7os"
      },
      "outputs": [],
      "source": [
        "# Oracle GP model with which we will compare our results\n",
        "import gpytorch\n",
        "\n",
        "class CustomizableGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, mean_module, base_kernel, likelihood):\n",
        "        super(CustomizableGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = mean_module\n",
        "        self.covar_module = base_kernel\n",
        "        self.likelihood = likelihood\n",
        "    def forward(self, x):\n",
        "        return gpytorch.distributions.MultivariateNormal(self.mean_module(x), self.covar_module(x))\n",
        "        \n",
        "\n",
        "# Define the modules\n",
        "mean_module = gpytorch.means.ConstantMean()\n",
        "base_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "\n",
        "#Setting the parameters of the model\n",
        "\n",
        "#mean_module.constant = torch.nn.Parameter(torch.tensor([mean_constant]))\n",
        "mean_module.constant = mean_constant\n",
        "base_kernel.base_kernel.lengthscale = lengthscale\n",
        "base_kernel.outputscale = outputscale\n",
        "likelihood.noise_covar.noise = noise\n",
        "\n",
        "\n",
        "oracle_model = CustomizableGPModel(None, None, mean_module, base_kernel, likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5C8EPY8dbbg_"
      },
      "outputs": [],
      "source": [
        "# Generating the distribution p(y_t|x_c;y_c;x_t) from the oracle model for the given context and target points\n",
        "\n",
        "def oracle_model_predict(oracle_model, likelihood, xc, yc, xt):\n",
        "    oracle_model.to(device)\n",
        "    likelihood.to(device)\n",
        "    oracle_model.set_train_data(inputs=xc, targets=yc, strict=False)\n",
        "\n",
        "    oracle_model.eval()\n",
        "    likelihood.eval()\n",
        "\n",
        "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "        posterior = likelihood(oracle_model(xt))\n",
        "        posterior_mean = posterior.mean\n",
        "        posterior_var = posterior.variance\n",
        "        return posterior_mean, posterior_var\n",
        "        \n",
        "# \"oracle_model_predict_B\" function is just for batching -  Generating the distribution p(y_t|x_c;y_c;x_t) from the oracle model for the given context and target points\n",
        "\n",
        "def oracle_model_predict_B(oracle_model, likelihood, xc, yc, xt, B):\n",
        "    means_B_oracle, vars_B_oracle = torch.zeros(B, xt.shape[1]), torch.zeros(B, xt.shape[1])\n",
        "    for b in range(B):\n",
        "        means_B_oracle[b, :], vars_B_oracle[b, :] = oracle_model_predict(\n",
        "            oracle_model, likelihood, xc[b, :, :], yc[b, :, 0], xt[b, :, :]\n",
        "        )\n",
        "    return means_B_oracle, vars_B_oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "class joint_prediction_machine:\n",
        "    def __init__(self, uq_model, device):\n",
        "        self.uq_model = uq_model\n",
        "        self.device = device\n",
        "\n",
        "    def joint_prediction(self, xc_data, yc_data, xt_data, T):\n",
        "        inner_context_x = []\n",
        "        inner_context_y = []\n",
        "        device = self.device\n",
        "        uq_model = self.uq_model\n",
        "        batch_size = xt_data.shape[0]\n",
        "        D = xt_data.shape[2]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(T):\n",
        "\n",
        "                # Generate random contexts\n",
        "                target_x = xt_data[:, i, :].unsqueeze(1)\n",
        "\n",
        "                inner_xc = torch.cat(inner_context_x, dim=1) if inner_context_x else torch.empty(batch_size, 0, D, device=device)\n",
        "                full_xc = torch.cat([xc_data, inner_xc], dim=1) \n",
        "                inner_yc = torch.cat(inner_context_y, dim=1) if inner_context_y else torch.empty(batch_size, 0, 1, device=device)\n",
        "                full_yc = torch.cat([yc_data, inner_yc], dim=1) \n",
        "                full_xc = full_xc.to(device)\n",
        "                full_yc = full_yc.to(device)\n",
        "                batch = SimpleNamespace(\n",
        "                    xc=full_xc,\n",
        "                    yc=full_yc,\n",
        "                    xt=target_x, \n",
        "                    yt=torch.zeros(batch_size, 1, 1, device=device)  \n",
        "                )\n",
        "\n",
        "\n",
        "                predicted_rewards = uq_model.predict(batch)\n",
        "                inner_context_x.append(target_x)\n",
        "                inner_context_y.append(predicted_rewards)\n",
        "\n",
        "        # Concatenate all inner context data\n",
        "        X = torch.cat(inner_context_x, dim=1)  \n",
        "        y = torch.cat(inner_context_y, dim=1)  \n",
        "\n",
        "        return X, y\n",
        "    \n",
        "    def marginal_prediction(self, xc_data, yc_data, xt_data, T):\n",
        "        device = self.device\n",
        "        uq_model = self.uq_model\n",
        "        batch_size = xt_data.shape[0]\n",
        "        D = xt_data.shape[2]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            batch = SimpleNamespace(\n",
        "                xc=xc_data,\n",
        "                yc=yc_data,\n",
        "                xt=xt_data, \n",
        "                yt=torch.zeros(batch_size, T, 1, device=device)  \n",
        "            )\n",
        "            predicted_rewards = uq_model.predict(batch) \n",
        "\n",
        "        return xt_data, predicted_rewards\n",
        "    \n",
        "    def permutation_prediction(self, eval_batch, P, T, marginal = False):\n",
        "        xc_data = eval_batch.xc\n",
        "        yc_data = eval_batch.yc\n",
        "        xt_raw = eval_batch.xt\n",
        "        B = xt_raw.shape[0]\n",
        "\n",
        "        y_P_BxTxD = torch.zeros(P, B, xt_raw.shape[1], yc_data.shape[2], device=self.device)\n",
        "\n",
        "        for p in range(P):\n",
        "            # Generate a random permutation for each batch\n",
        "            perm_indices = torch.randperm(xt_raw.shape[1])\n",
        "            xt_data = xt_raw[:, perm_indices, :]\n",
        "            if marginal:\n",
        "                X, y = self.marginal_prediction(xc_data, yc_data, xt_data, T)\n",
        "            else:\n",
        "                X, y = self.joint_prediction(xc_data, yc_data, xt_data, T)\n",
        "            # Permute back to original order\n",
        "            y = y[:, perm_indices.argsort(), :]\n",
        "            y = y.unsqueeze(0)\n",
        "            y_P_BxTxD[p, :, :, :] = y\n",
        "\n",
        "        return y_P_BxTxD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimating_losses(batch,\n",
        "                      B, N, T, P, D,\n",
        "                      y_P_BxTxD_list, context_continuous,\n",
        "                      oracle_model, likelihood):\n",
        "\n",
        "\n",
        "    P = y_P_BxTxD_list[0].shape[0]\n",
        "\n",
        "    a1_loss_list = []\n",
        "    a2_loss_list = []\n",
        "\n",
        "    for y_P_BxTxD in y_P_BxTxD_list:\n",
        "\n",
        "        a1_loss = 0\n",
        "        a2_loss = 0\n",
        "        b_loss = 0\n",
        "\n",
        "        # for every permutation\n",
        "        for p in range(P):\n",
        "            # for filling in samples under oracle pps\n",
        "            yt_true_oracle_BxT = torch.zeros(B, T).to(device)\n",
        "\n",
        "            # for every step\n",
        "            for t in range(T):\n",
        "\n",
        "                # add xt[:, :step, :] to xc to determine the full oracle context x at this step - resulting dimension is [N+step,1]\n",
        "                if context_continuous:\n",
        "                    curr_xc_BxNptxD = torch.cat([batch.xc, batch.xt[:, :t, :]], dim=1)\n",
        "                    #add output_tnp_PxTxD[p, :step, :] to yc to determine the full oracle context y at this step -\n",
        "                    #resulting dimension is [N+step]\n",
        "                    curr_yc_BxNpt= torch.cat([batch.yc, y_P_BxTxD[p, :, :t, :]], dim=1).squeeze(2)\n",
        "                    assert curr_xc_BxNptxD.shape == (B, N + t, D)\n",
        "                    assert curr_yc_BxNpt.shape == (B, N + t)\n",
        "\n",
        "                else:\n",
        "                    # here the context stays the same - evaluate martingale property\n",
        "                    curr_xc_BxNptxD = batch.xc # torch.cat([batch.xc, batch.xt[:, :t, :]], dim=1)\n",
        "                    curr_yc_BxNpt= batch.yc.squeeze(2)\n",
        "                    assert curr_xc_BxNptxD.shape == (B, N, D)\n",
        "                    assert curr_yc_BxNpt.shape == (B, N)\n",
        "\n",
        "                # the next xt value to be predicted - dim in [1] for both of them\n",
        "                curr_xt_BxD = batch.xt[:, t, :]\n",
        "                # the point that will be evaluated under the oracle pp\n",
        "                curr_yt_tnp_B = y_P_BxTxD[p, :, t, 0].to(device)\n",
        "\n",
        "                means_B_oracle, vars_B_oracle = oracle_model_predict_B(\n",
        "                    oracle_model,\n",
        "                    likelihood,\n",
        "                    curr_xc_BxNptxD,\n",
        "                    curr_yc_BxNpt.unsqueeze(2),\n",
        "                    curr_xt_BxD.unsqueeze(1),\n",
        "                    B\n",
        "                )\n",
        "                means_B_oracle, sds_B_oracle = (\n",
        "                    means_B_oracle.squeeze(1).to(device),\n",
        "                    torch.sqrt(vars_B_oracle).squeeze(1).to(device)\n",
        "                )\n",
        "\n",
        "                # scenario A1, y_hat is from tnp, evaluated on oracle(tnp_trajectory)\n",
        "                distributions = Normal(means_B_oracle, sds_B_oracle)\n",
        "                log_probs = distributions.log_prob(curr_yt_tnp_B)\n",
        "                a1_loss += log_probs.mean()\n",
        "\n",
        "                # scenario A2, y_hat is from oracle, evaluated on oracle(tnp_trajectory)\n",
        "                samples_B = torch.normal(means_B_oracle, sds_B_oracle)\n",
        "                log_probs = distributions.log_prob(samples_B)\n",
        "                a2_loss += log_probs.mean()\n",
        "\n",
        "        a1_loss_list.append(a1_loss.detach().cpu() / (P * T))\n",
        "        a2_loss_list.append(a2_loss.detach().cpu() / (P * T))  \n",
        "                #------------------------------------EVALUATION OF TNP ENDS HERE ---------------------------------------\n",
        "\n",
        "\n",
        "            #-------------------------------------WE START EVALUATING BENCHMARK AFTER THIS -------------------------\n",
        "\n",
        "\n",
        "    # Part B: Oracle Baseline (Single Trajectory)\n",
        "    b_loss = 0\n",
        "    yt_true_oracle_BxT = torch.zeros(B, T).to(device)\n",
        "    \n",
        "    # Single trajectory for oracle baseline\n",
        "    for t in range(T):\n",
        "        if context_continuous:\n",
        "            curr_xc_BxNptxD = torch.cat([batch.xc, batch.xt[:, :t, :]], dim=1)\n",
        "            curr_yc_oracle_BxNpt = torch.cat([batch.yc.squeeze(2), yt_true_oracle_BxT[:, :t]], dim=1)\n",
        "        else:\n",
        "            curr_xc_BxNptxD = batch.xc\n",
        "            curr_yc_oracle_BxNpt = batch.yc.squeeze(2)\n",
        "\n",
        "        curr_xt_BxD = batch.xt[:, t, :]\n",
        "\n",
        "        means_B_oracle_oracle, vars_B_oracle_oracle = oracle_model_predict_B(\n",
        "            oracle_model,\n",
        "            likelihood,\n",
        "            curr_xc_BxNptxD,\n",
        "            curr_yc_oracle_BxNpt.unsqueeze(2),\n",
        "            curr_xt_BxD.unsqueeze(1),\n",
        "            B\n",
        "        )\n",
        "        means_B_oracle_oracle, sds_B_oracle_oracle = (\n",
        "            means_B_oracle_oracle.squeeze(1),\n",
        "            torch.sqrt(vars_B_oracle_oracle).squeeze(1)\n",
        "        )\n",
        "\n",
        "        # Generate and evaluate single oracle trajectory\n",
        "        distributions = Normal(means_B_oracle_oracle, sds_B_oracle_oracle)\n",
        "        samples_B = torch.normal(means_B_oracle_oracle, sds_B_oracle_oracle)\n",
        "        yt_true_oracle_BxT[:, t] = samples_B\n",
        "        log_probs = distributions.log_prob(samples_B)\n",
        "        b_loss += log_probs.mean()\n",
        "        \n",
        "    b_loss = b_loss.detach().cpu() / T\n",
        "\n",
        "    return a1_loss_list, a2_loss_list, b_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_batch(original_batch, N, T):\n",
        "    return SimpleNamespace(\n",
        "        xc = original_batch.x[:, :N, :].clone().to(device),\n",
        "        yc = original_batch.y[:, :N, :].clone().to(device),\n",
        "        xt = original_batch.x[:, N:T+N, :].clone().to(device),\n",
        "        yt = original_batch.y[:, N:T+N, :].clone().to(device)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First experiment: Joint loss - increasing context\n",
        "P = 50\n",
        "B = 64\n",
        "D = 4\n",
        "test_horizon = 2000\n",
        "\n",
        "def collect_joint_context_data(eval_batch, start, end, step):\n",
        "    T = 10\n",
        "    auto_reg_a1_list = []\n",
        "    excg_a1_list = []\n",
        "    excg_marginal_a1_list = []\n",
        "    oracle_list = []\n",
        "    auto_reg_a2_list = []\n",
        "    excg_a2_list = []\n",
        "    excg_marginal_a2_list = []\n",
        "    for n in tqdm(range(start, end, step)):\n",
        "        batch = create_batch(eval_batch, n, T)\n",
        "\n",
        "        auto_reg_y_P_BxTxD = auto_reg_joint_prediction_machine.permutation_prediction(batch, P, T, marginal=False)\n",
        "        excg_y_P_BxTxD = excg_joint_prediction_machine.permutation_prediction(batch, P, T, marginal=False)\n",
        "        excg_marginal_y_P_BxTxD = excg_joint_prediction_machine.permutation_prediction(batch, P, T, marginal=True)\n",
        "\n",
        "        y_P_BxTxD_list = [auto_reg_y_P_BxTxD, excg_y_P_BxTxD, excg_marginal_y_P_BxTxD]\n",
        "\n",
        "        a1_list, a2_list, oracle_loss = estimating_losses(\n",
        "            batch, B, n, T, P, D, y_P_BxTxD_list, context_continuous,\n",
        "            oracle_model, likelihood)\n",
        "        \n",
        "        auto_reg_a1_list.append(a1_list[0])\n",
        "        excg_a1_list.append(a1_list[1])\n",
        "        excg_marginal_a1_list.append(a1_list[2])\n",
        "        oracle_list.append(oracle_loss)\n",
        "        auto_reg_a2_list.append(a2_list[0])\n",
        "        excg_a2_list.append(a2_list[1])\n",
        "        excg_marginal_a2_list.append(a2_list[2])\n",
        "\n",
        "    return auto_reg_a1_list, excg_a1_list, excg_marginal_a1_list, oracle_list, auto_reg_a2_list, excg_a2_list, excg_marginal_a2_list\n",
        "\n",
        "# Second experiment: Joint loss - increasing target\n",
        "def collect_joint_target_data(eval_batch, start, end, step):\n",
        "    N = 10\n",
        "    auto_reg_a1_list = []\n",
        "    excg_a1_list = []\n",
        "    excg_marginal_a1_list = []\n",
        "    oracle_list = []\n",
        "    auto_reg_a2_list = []\n",
        "    excg_a2_list = []\n",
        "    excg_marginal_a2_list = []\n",
        "    \n",
        "    for t in tqdm(range(start, end, step)):\n",
        "        batch = create_batch(eval_batch, N, t)\n",
        "\n",
        "        auto_reg_y_P_BxTxD = auto_reg_joint_prediction_machine.permutation_prediction(batch, P, t, marginal=False)\n",
        "        excg_y_P_BxTxD = excg_joint_prediction_machine.permutation_prediction(batch, P, t, marginal=False)\n",
        "        excg_marginal_y_P_BxTxD = excg_joint_prediction_machine.permutation_prediction(batch, P, t, marginal=True)\n",
        "\n",
        "        y_P_BxTxD_list = [auto_reg_y_P_BxTxD, excg_y_P_BxTxD, excg_marginal_y_P_BxTxD]\n",
        "\n",
        "        a1_list, a2_list, oracle_loss = estimating_losses(\n",
        "            batch, B, N, t, P, D, y_P_BxTxD_list, context_continuous,\n",
        "            oracle_model, likelihood)\n",
        "        \n",
        "        auto_reg_a1_list.append(a1_list[0])\n",
        "        excg_a1_list.append(a1_list[1])\n",
        "        excg_marginal_a1_list.append(a1_list[2])\n",
        "        oracle_list.append(oracle_loss)\n",
        "        auto_reg_a2_list.append(a2_list[0])\n",
        "        excg_a2_list.append(a2_list[1])\n",
        "        excg_marginal_a2_list.append(a2_list[2])\n",
        "    \n",
        "    return auto_reg_a1_list, excg_a1_list, excg_marginal_a1_list, oracle_list, auto_reg_a2_list, excg_a2_list, excg_marginal_a2_list\n",
        "\n",
        "# Third experiment: Marginal loss - increasing context\n",
        "def collect_marginal_context_data(eval_batch, start, end, step):\n",
        "    T = 1\n",
        "    auto_reg_a1_list = []\n",
        "    excg_a1_list = []\n",
        "    excg_marginal_a1_list = []\n",
        "    oracle_list = []\n",
        "    auto_reg_a2_list = []\n",
        "    excg_a2_list = []\n",
        "    excg_marginal_a2_list = []\n",
        "    \n",
        "    for n in tqdm(range(start, end, step)):\n",
        "        batch = create_batch(eval_batch, n, T)\n",
        "\n",
        "        auto_reg_y_P_BxTxD = auto_reg_joint_prediction_machine.permutation_prediction(batch, P, T, marginal=False)\n",
        "        excg_y_P_BxTxD = excg_joint_prediction_machine.permutation_prediction(batch, P, T, marginal=False)\n",
        "        excg_marginal_y_P_BxTxD = excg_joint_prediction_machine.permutation_prediction(batch, P, T, marginal=True)\n",
        "\n",
        "        y_P_BxTxD_list = [auto_reg_y_P_BxTxD, excg_y_P_BxTxD, excg_marginal_y_P_BxTxD]\n",
        "        \n",
        "\n",
        "        a1_list, a2_list, oracle_loss = estimating_losses(\n",
        "            batch, B, n, T, P, D, y_P_BxTxD_list, context_continuous,\n",
        "            oracle_model, likelihood)\n",
        "        \n",
        "        auto_reg_a1_list.append(a1_list[0])\n",
        "        excg_a1_list.append(a1_list[1])\n",
        "        excg_marginal_a1_list.append(a1_list[2])\n",
        "        oracle_list.append(oracle_loss)\n",
        "        auto_reg_a2_list.append(a2_list[0])\n",
        "        excg_a2_list.append(a2_list[1])\n",
        "        excg_marginal_a2_list.append(a2_list[2])\n",
        "    \n",
        "    return auto_reg_a1_list, excg_a1_list, excg_marginal_a1_list, oracle_list, auto_reg_a2_list, excg_a2_list, excg_marginal_a2_list\n",
        "\n",
        "# Initialize models and dataset\n",
        "auto_reg_joint_prediction_machine = joint_prediction_machine(auto_reg_model, device)\n",
        "excg_joint_prediction_machine = joint_prediction_machine(excg_model, device)\n",
        "\n",
        "permute = True\n",
        "context_continuous = True\n",
        "\n",
        "eval_dataset = GPSamplerConstantDataset(\n",
        "    num_samples=num_test_samples, \n",
        "    noise=noise,\n",
        "    dimension=dim_llm_embedding, \n",
        "    horizon=test_horizon)\n",
        "\n",
        "eval_data_loader = DataLoader(eval_dataset, batch_size=B, shuffle=True, collate_fn=scalar_collate_fn)\n",
        "eval_batch = next(iter(eval_data_loader))\n",
        "\n",
        "start, end, step = 1, 31, 5\n",
        "\n",
        "# Collect data from all experiments\n",
        "print(\"Collecting Joint Context Data...\")\n",
        "joint_context_data = collect_joint_context_data(eval_batch, start, end, step)\n",
        "\n",
        "print(\"Collecting Joint Target Data...\")\n",
        "joint_target_data = collect_joint_target_data(eval_batch, start, end, step)\n",
        "\n",
        "print(\"Collecting Marginal Context Data...\")\n",
        "marginal_context_data = collect_marginal_context_data(eval_batch, start, end, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "def create_combined_plots(start, end, step,\n",
        "                        joint_context_data, joint_target_data, marginal_context_data):\n",
        "    \"\"\"\n",
        "    Create combined plots for joint and marginal predictions with their differences\n",
        "    \"\"\"\n",
        "    # Create figure with GridSpec\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = gridspec.GridSpec(2, 3, figure=fig)\n",
        "    \n",
        "    # Unpack data\n",
        "    auto_reg_a1_list_jc, excg_a1_list_jc, excg_marginal_a1_list_jc, oracle_list_jc, auto_reg_a2_list_jc, excg_a2_list_jc, excg_marginal_a2_list_jc = joint_context_data\n",
        "    auto_reg_a1_list_jt, excg_a1_list_jt, excg_marginal_a1_list_jt, oracle_list_jt, auto_reg_a2_list_jt, excg_a2_list_jt, excg_marginal_a2_list_jt = joint_target_data\n",
        "    auto_reg_a1_list_mc, excg_a1_list_mc, excg_marginal_a1_list_mc, oracle_list_mc, auto_reg_a2_list_mc, excg_a2_list_mc, excg_marginal_a2_list_mc = marginal_context_data\n",
        "    \n",
        "    # Top row - Perplexity plots\n",
        "    # Joint Context\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.plot(range(start, end, step), auto_reg_a1_list_jc, label=\"AutoReg a1\", c='green', linewidth=2)\n",
        "    ax1.plot(range(start, end, step), excg_a1_list_jc, label=\"ExcG a1\", c='blue', linewidth=2)\n",
        "    ax1.plot(range(start, end, step), excg_marginal_a1_list_jc, label=\"PFN a1\", c='red', linewidth=2)\n",
        "    ax1.plot(range(start, end, step), oracle_list_jc, label=\"Oracle\", c='black', linewidth=2)\n",
        "    ax1.plot(range(start, end, step), auto_reg_a2_list_jc, label=\"AutoReg a2\", c='green', linewidth=2, linestyle='--')\n",
        "    ax1.plot(range(start, end, step), excg_a2_list_jc, label=\"ExcG a2\", c='blue', linewidth=2, linestyle='--')\n",
        "    ax1.plot(range(start, end, step), excg_marginal_a2_list_jc, label=\"PFN a2\", c='red', linewidth=2, linestyle='--')\n",
        "    ax1.set_title('Joint Prediction: Fixed Target (10)\\nContext Points vs Perplexity', fontsize=12)\n",
        "    ax1.set_xlabel('Number of Context Points', fontsize=10)\n",
        "    ax1.set_ylabel('Perplexity', fontsize=10)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax1.legend()\n",
        "\n",
        "    # Joint Target\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.plot(range(start, end, step), auto_reg_a1_list_jt, label=\"AutoReg\", c='green', linewidth=2)\n",
        "    ax2.plot(range(start, end, step), excg_a1_list_jt, label=\"ExcG\", c='blue', linewidth=2)\n",
        "    ax2.plot(range(start, end, step), excg_marginal_a1_list_jt, label=\"PFN\", c='red', linewidth=2)\n",
        "    ax2.plot(range(start, end, step), oracle_list_jt, label=\"Oracle\", c='black', linewidth=2)\n",
        "    ax2.plot(range(start, end, step), auto_reg_a2_list_jt, label=\"AutoReg a2\", c='green', linewidth=2, linestyle='--')\n",
        "    ax2.plot(range(start, end, step), excg_a2_list_jt, label=\"ExcG a2\", c='blue', linewidth=2, linestyle='--')\n",
        "    ax2.plot(range(start, end, step), excg_marginal_a2_list_jt, label=\"PFN a2\", c='red', linewidth=2, linestyle='--')\n",
        "    ax2.set_title('Joint Prediction: Fixed Context (10)\\nTarget Points vs Perplexity', fontsize=12)\n",
        "    ax2.set_xlabel('Number of Target Points', fontsize=10)\n",
        "    ax2.set_ylabel('Perplexity', fontsize=10)\n",
        "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax2.legend()\n",
        "\n",
        "    # Marginal Context\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    ax3.plot(range(start, end, step), auto_reg_a1_list_mc, label=\"AutoReg\", c='green', linewidth=2)\n",
        "    ax3.plot(range(start, end, step), excg_a1_list_mc, label=\"ExcG\", c='blue', linewidth=2)\n",
        "    ax3.plot(range(start, end, step), excg_marginal_a1_list_mc, label=\"PFN\", c='red', linewidth=2)\n",
        "    ax3.plot(range(start, end, step), oracle_list_mc, label=\"Oracle\", c='black', linewidth=2)\n",
        "    ax3.plot(range(start, end, step), auto_reg_a2_list_mc, label=\"AutoReg a2\", c='green', linewidth=2, linestyle='--')\n",
        "    ax3.plot(range(start, end, step), excg_a2_list_mc, label=\"ExcG a2\", c='blue', linewidth=2, linestyle='--')\n",
        "    ax3.plot(range(start, end, step), excg_marginal_a2_list_mc, label=\"PFN a2\", c='red', linewidth=2, linestyle='--')\n",
        "    ax3.set_title('Marginal Prediction: Fixed Target (1)\\nContext Points vs Perplexity', fontsize=12)\n",
        "    ax3.set_xlabel('Number of Context Points', fontsize=10)\n",
        "    ax3.set_ylabel('Perplexity', fontsize=10)\n",
        "    ax3.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax3.legend()\n",
        "\n",
        "    # Bottom row - Difference plots\n",
        "    # Joint Context Difference\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    auto_reg_diff_jc = [(b - a) for a, b in zip(auto_reg_a1_list_jc, oracle_list_jc)]\n",
        "    excg_diff_jc = [(b - a) for a, b in zip(excg_a1_list_jc, oracle_list_jc)]\n",
        "    excg_marginal_diff_jc = [(b - a) for a, b in zip(excg_marginal_a1_list_jc, oracle_list_jc)]\n",
        "    auto_reg_diff_a2_jc = [(b - a) for a, b in zip(auto_reg_a2_list_jc, oracle_list_jc)]\n",
        "    excg_diff_a2_jc = [(b - a) for a, b in zip(excg_a2_list_jc, oracle_list_jc)]\n",
        "    excg_marginal_diff_a2_jc = [(b - a) for a, b in zip(excg_marginal_a2_list_jc, oracle_list_jc)]\n",
        "    ax4.plot(range(start, end, step), auto_reg_diff_jc, label=\"AutoReg Diff\", c='green', linewidth=2)\n",
        "    ax4.plot(range(start, end, step), excg_diff_jc, label=\"ExcG Diff\", c='blue', linewidth=2)\n",
        "    ax4.plot(range(start, end, step), excg_marginal_diff_jc, label=\"PFN Diff\", c='red', linewidth=2)\n",
        "    ax4.plot(range(start, end, step), auto_reg_diff_a2_jc, label=\"AutoReg a2 Diff\", c='green', linewidth=2, linestyle='--')\n",
        "    ax4.plot(range(start, end, step), excg_diff_a2_jc, label=\"ExcG a2 Diff\", c='blue', linewidth=2, linestyle='--')\n",
        "    ax4.plot(range(start, end, step), excg_marginal_diff_a2_jc, label=\"PFN a2 Diff\", c='red', linewidth=2, linestyle='--')\n",
        "    ax4.set_title('Joint Prediction: Fixed Target (10)\\nDifference from Oracle', fontsize=12)\n",
        "    ax4.set_xlabel('Number of Context Points', fontsize=10)\n",
        "    ax4.set_ylabel('Perplexity Difference', fontsize=10)\n",
        "    ax4.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax4.legend()\n",
        "\n",
        "    # Joint Target Difference\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    auto_reg_diff_jt = [(b - a) for a, b in zip(auto_reg_a1_list_jt, oracle_list_jt)]\n",
        "    excg_diff_jt = [(b - a) for a, b in zip(excg_a1_list_jt, oracle_list_jt)]\n",
        "    excg_marginal_diff_jt = [(b - a) for a, b in zip(excg_marginal_a1_list_jt, oracle_list_jt)]\n",
        "    auto_reg_diff_a2_jt = [(b - a) for a, b in zip(auto_reg_a2_list_jt, oracle_list_jt)]\n",
        "    excg_diff_a2_jt = [(b - a) for a, b in zip(excg_a2_list_jt, oracle_list_jt)]\n",
        "    excg_marginal_diff_a2_jt = [(b - a) for a, b in zip(excg_marginal_a2_list_jt, oracle_list_jt)]\n",
        "    ax5.plot(range(start, end, step), auto_reg_diff_jt, label=\"AutoReg Diff\", c='green', linewidth=2)\n",
        "    ax5.plot(range(start, end, step), excg_diff_jt, label=\"ExcG Diff\", c='blue', linewidth=2)\n",
        "    ax5.plot(range(start, end, step), excg_marginal_diff_jt, label=\"PFN Diff\", c='red', linewidth=2)\n",
        "    ax5.plot(range(start, end, step), auto_reg_diff_a2_jt, label=\"AutoReg a2 Diff\", c='green', linewidth=2, linestyle='--')\n",
        "    ax5.plot(range(start, end, step), excg_diff_a2_jt, label=\"ExcG a2 Diff\", c='blue', linewidth=2, linestyle='--')\n",
        "    ax5.plot(range(start, end, step), excg_marginal_diff_a2_jt, label=\"PFN a2 Diff\", c='red', linewidth=2, linestyle='--')\n",
        "    ax5.set_title('Joint Prediction: Fixed Context (10)\\nDifference from Oracle', fontsize=12)\n",
        "    ax5.set_xlabel('Number of Target Points', fontsize=10)\n",
        "    ax5.set_ylabel('Perplexity Difference', fontsize=10)\n",
        "    ax5.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax5.legend()\n",
        "\n",
        "    # Marginal Context Difference\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    auto_reg_diff_mc = [(b - a) for a, b in zip(auto_reg_a1_list_mc, oracle_list_mc)]\n",
        "    excg_diff_mc = [(b - a) for a, b in zip(excg_a1_list_mc, oracle_list_mc)]\n",
        "    excg_marginal_diff_mc = [(b - a) for a, b in zip(excg_marginal_a1_list_mc, oracle_list_mc)]\n",
        "    auto_reg_diff_a2_mc = [(b - a) for a, b in zip(auto_reg_a2_list_mc, oracle_list_mc)]\n",
        "    excg_diff_a2_mc = [(b - a) for a, b in zip(excg_a2_list_mc, oracle_list_mc)]\n",
        "    excg_marginal_diff_a2_mc = [(b - a) for a, b in zip(excg_marginal_a2_list_mc, oracle_list_mc)]\n",
        "    ax6.plot(range(start, end, step), auto_reg_diff_mc, label=\"AutoReg Diff\", c='green', linewidth=2)\n",
        "    ax6.plot(range(start, end, step), excg_diff_mc, label=\"ExcG Diff\", c='blue', linewidth=2)\n",
        "    ax6.plot(range(start, end, step), excg_marginal_diff_mc, label=\"PFN Diff\", c='red', linewidth=2)\n",
        "    ax6.plot(range(start, end, step), auto_reg_diff_a2_mc, label=\"AutoReg a2 Diff\", c='green', linewidth=2, linestyle='--')\n",
        "    ax6.plot(range(start, end, step), excg_diff_a2_mc, label=\"ExcG a2 Diff\", c='blue', linewidth=2, linestyle='--')\n",
        "    ax6.plot(range(start, end, step), excg_marginal_diff_a2_mc, label=\"PFN a2 Diff\", c='red', linewidth=2, linestyle='--')\n",
        "    ax6.set_title('Marginal Prediction: Fixed Target (1)\\nDifference from Oracle', fontsize=12)\n",
        "    ax6.set_xlabel('Number of Context Points', fontsize=10)\n",
        "    ax6.set_ylabel('Perplexity Difference', fontsize=10)\n",
        "    ax6.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax6.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'model_checkpoint_{Checkpoint_iter}.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the combined plots using the function from the previous code\n",
        "create_combined_plots(start, end, step,\n",
        "                     joint_context_data,\n",
        "                     joint_target_data,\n",
        "                     marginal_context_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
